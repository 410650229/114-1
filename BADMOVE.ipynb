{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e9b85-6815-44cf-8a83-35594acdaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0052018c-8607-42f7-afd5-7a526274c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影片檔案: texture_video.avi\n",
      "FPS: 30.0\n",
      "畫素 (寬 x 高): 800 x 600\n",
      "總幀數: 5437\n",
      "影片秒數: 181.23 秒\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 讀取影片\n",
    "video_path = 'texture_video.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"無法開啟影片檔案。\")\n",
    "else:\n",
    "    # 獲取影片資訊\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # 幀速率\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 總幀數\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # 影像寬度\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 影像高度\n",
    "    video_duration = frame_count / fps  # 影片秒數\n",
    "\n",
    "    # 輸出資訊\n",
    "    print(f\"影片檔案: {video_path}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"畫素 (寬 x 高): {width} x {height}\")\n",
    "    print(f\"總幀數: {frame_count}\")\n",
    "    print(f\"影片秒數: {video_duration:.2f} 秒\")\n",
    "\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f79f6bc-7687-4c15-b71b-8990aaf59ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'image1.png', 'width': 512, 'height': 512, 'resolution': '512 x 512'}\n",
      "{'path': 'image2.png', 'width': 512, 'height': 512, 'resolution': '512 x 512'}\n",
      "{'path': 'image1_groundtruth.png', 'width': 512, 'height': 512, 'resolution': '512 x 512'}\n",
      "{'path': 'image2_groundtruth.png', 'width': 512, 'height': 512, 'resolution': '512 x 512'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def get_image_info(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return f\"Error: Unable to load image from {image_path}\"\n",
    "\n",
    "    # 取得解析度\n",
    "    resolution = image.shape\n",
    "    height, width = resolution[:2]\n",
    "\n",
    "    return {\n",
    "        \"path\": image_path,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"resolution\": f\"{width} x {height}\"\n",
    "    }\n",
    "\n",
    "# 定義影像路徑\n",
    "image_paths = [\"image1.png\", \"image2.png\", \"image1_groundtruth.png\", \"image2_groundtruth.png\"]\n",
    "\n",
    "# 逐一處理影像並回報資訊\n",
    "for path in image_paths:\n",
    "    info = get_image_info(path)\n",
    "    print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e20edf00-c507-441e-98b1-4b421519e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color distribution for image1_groundtruth.png:\n",
      "{'Blue': {0: 9538, 255: 252606}, 'Green': {0: 9538, 255: 252606}, 'Red': {0: 9538, 255: 252606}}\n",
      "\n",
      "Color distribution for image2_groundtruth.png:\n",
      "{'Blue': {0: 6122, 255: 256022}, 'Green': {0: 6122, 255: 256022}, 'Red': {0: 6122, 255: 256022}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    \"\"\"\n",
    "    分析影像中的顏色分佈情況。\n",
    "    \n",
    "    參數:\n",
    "        image_path (str): 影像的檔案路徑。\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含各顏色（灰階或 RGB 值）及其對應像素數量的字典。\n",
    "    \"\"\"\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return f\"Error: Unable to load image from {image_path}\"\n",
    "    \n",
    "    # 檢查影像是否為灰階\n",
    "    if len(image.shape) == 2:  # 單通道灰階影像\n",
    "        unique, counts = np.unique(image, return_counts=True)\n",
    "        color_distribution = dict(zip(unique, counts))\n",
    "    else:  # RGB 或其他多通道影像\n",
    "        # 將每個通道分開處理\n",
    "        color_distribution = {}\n",
    "        for channel, color_name in enumerate([\"Blue\", \"Green\", \"Red\"]):\n",
    "            unique, counts = np.unique(image[:, :, channel], return_counts=True)\n",
    "            color_distribution[color_name] = dict(zip(unique, counts))\n",
    "    \n",
    "    return color_distribution\n",
    "\n",
    "# 測試影像路徑\n",
    "groundtruth_images = [\"image1_groundtruth.png\", \"image2_groundtruth.png\"]\n",
    "\n",
    "# 分析並輸出每張影像的顏色分佈\n",
    "for image_path in groundtruth_images:\n",
    "    result = analyze_image_colors(image_path)\n",
    "    print(f\"Color distribution for {image_path}:\")\n",
    "    print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37c89a1-e277-4075-a04a-c2b1622a8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 461ms/step - loss: 0.6818\n",
      "Epoch 2/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 465ms/step - loss: 0.6781\n",
      "Epoch 3/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 502ms/step - loss: 0.6780\n",
      "Epoch 4/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 512ms/step - loss: 0.6776\n",
      "Epoch 5/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 507ms/step - loss: 0.6773\n",
      "Epoch 6/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 486ms/step - loss: 0.6773\n",
      "Epoch 7/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 491ms/step - loss: 0.6772\n",
      "Epoch 8/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 479ms/step - loss: 0.6770\n",
      "Epoch 9/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - loss: 0.6770\n",
      "Epoch 10/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 478ms/step - loss: 0.6769\n",
      "Epoch 11/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - loss: 0.6768\n",
      "Epoch 12/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 483ms/step - loss: 0.6767\n",
      "Epoch 13/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 485ms/step - loss: 0.6767\n",
      "Epoch 14/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 488ms/step - loss: 0.6766\n",
      "Epoch 15/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 503ms/step - loss: 0.6765\n",
      "Epoch 16/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 483ms/step - loss: 0.6765\n",
      "Epoch 17/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - loss: 0.6765\n",
      "Epoch 18/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - loss: 0.6764\n",
      "Epoch 19/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - loss: 0.6764\n",
      "Epoch 20/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 476ms/step - loss: 0.6763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x281668f8e00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 載入影片檔案\n",
    "cap = cv2.VideoCapture('texture_video.avi')\n",
    "frames = []\n",
    "\n",
    "# 讀取前720幀影像\n",
    "for i in range(720):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 轉為灰階\n",
    "        frame = cv2.resize(frame, (800, 600))  # 調整大小\n",
    "        frames.append(frame)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# 轉為 NumPy 陣列，並正規化\n",
    "frames = np.array(frames) / 255.0\n",
    "frames = frames.reshape(-1, 800, 600, 1)  # 調整為 (幀數, 高, 寬, 測量通道數)\n",
    "\n",
    "# 定義 Autoencoder 模型\n",
    "def create_autoencoder():\n",
    "    input_img = layers.Input(shape=(800, 600, 1))\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # Decoder\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    return models.Model(input_img, decoded)\n",
    "\n",
    "# 建立模型\n",
    "autoencoder = create_autoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# 訓練模型\n",
    "autoencoder.fit(frames, frames, epochs=20, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca2821a9-9954-4c39-b686-c11f4ac950be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Image 1 Confusion Matrix: {'TP': 0, 'FP': 0, 'TN': 9538, 'FN': 252606}\n",
      "Image 2 Confusion Matrix: {'TP': 0, 'FP': 0, 'TN': 6122, 'FN': 256022}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 動態讀取影像並獲取資訊\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # 獲取影像資訊\n",
    "    image_info = get_image_info(image_path)\n",
    "    if isinstance(image_info, str):  # 錯誤訊息處理\n",
    "        raise ValueError(image_info)\n",
    "\n",
    "    # 根據解析度讀取與處理影像\n",
    "    width = image_info[\"width\"]\n",
    "    height = image_info[\"height\"]\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (width, height))  # 確保讀取後調整大小一致\n",
    "    image = image / 255.0  # 正規化\n",
    "    return image.reshape(1, height, width, 1)  # 返回形狀為 (1, 高, 寬, 通道)\n",
    "\n",
    "# 加載影像和 Groundtruth\n",
    "image1 = load_and_preprocess_image(\"image1.png\")\n",
    "image2 = load_and_preprocess_image(\"image2.png\")\n",
    "gt_image1 = load_and_preprocess_image(\"image1_groundtruth.png\").astype(int)\n",
    "gt_image2 = load_and_preprocess_image(\"image2_groundtruth.png\").astype(int)\n",
    "\n",
    "# 使用 Autoencoder 預測影像\n",
    "reconstructed_image1 = autoencoder.predict(image1)\n",
    "reconstructed_image2 = autoencoder.predict(image2)\n",
    "\n",
    "# 計算像素差異\n",
    "threshold =10  # 閾值\n",
    "diff_image1 = np.abs(image1 - reconstructed_image1) > threshold\n",
    "diff_image2 = np.abs(image2 - reconstructed_image2) > threshold\n",
    "\n",
    "# 計算混淆矩陣\n",
    "def calculate_confusion_matrix(groundtruth, prediction):\n",
    "    TP = np.sum((groundtruth == 1) & (prediction == 1))\n",
    "    FP = np.sum((groundtruth == 0) & (prediction == 1))\n",
    "    TN = np.sum((groundtruth == 0) & (prediction == 0))\n",
    "    FN = np.sum((groundtruth == 1) & (prediction == 0))\n",
    "    return {\"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN}\n",
    "\n",
    "# 計算 Image 1 和 Image 2 的混淆矩陣\n",
    "conf_matrix1 = calculate_confusion_matrix(gt_image1[0, :, :, 0], diff_image1[0, :, :, 0])\n",
    "conf_matrix2 = calculate_confusion_matrix(gt_image2[0, :, :, 0], diff_image2[0, :, :, 0])\n",
    "\n",
    "# 輸出混淆矩陣結果\n",
    "print(\"Image 1 Confusion Matrix:\", conf_matrix1)\n",
    "print(\"Image 2 Confusion Matrix:\", conf_matrix2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b9eb9b4-3dd8-4dac-8781-c890f3aa393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n"
     ]
    }
   ],
   "source": [
    "# 載入測試影像\n",
    "def load_and_preprocess_image(filename):\n",
    "    img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (800, 600))\n",
    "    img = img / 255.0\n",
    "    return img.reshape(1, 800, 600, 1)\n",
    "\n",
    "image1 = load_and_preprocess_image('image1.png')\n",
    "image2 = load_and_preprocess_image('image2.png')\n",
    "\n",
    "# 使用 Autoencoder 進行重建\n",
    "reconstructed_image1 = autoencoder.predict(image1)\n",
    "reconstructed_image2 = autoencoder.predict(image2)\n",
    "\n",
    "# 計算像素差異\n",
    "threshold = 0.2  # 設定像素差異閾值\n",
    "diff_image1 = np.abs(image1 - reconstructed_image1) > threshold\n",
    "diff_image2 = np.abs(image2 - reconstructed_image2) > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aaeee5c-c99c-41f2-8101-d30d2d79c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_groundtruth(filename):\n",
    "    img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (600, 800))  # 確保與 prediction 的形狀一致\n",
    "    return (img == 0).astype(int)  # 黑色為瑕疵，轉為二值化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c80a402f-42f0-42f2-abdb-297a23ce82bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 Confusion Matrix:\n",
      "TP: 0, FP: 0, TN: 463666, FN: 16334\n",
      "Image 2 Confusion Matrix:\n",
      "TP: 0, FP: 0, TN: 469190, FN: 10810\n"
     ]
    }
   ],
   "source": [
    "# 載入 Groundtruth\n",
    "gt_image1 = load_groundtruth('image1_groundtruth.png')\n",
    "gt_image2 = load_groundtruth('image2_groundtruth.png')\n",
    "\n",
    "# 計算混淆矩陣\n",
    "tp1, fp1, tn1, fn1 = calculate_confusion_matrix(gt_image1, diff_image1[0, :, :, 0])\n",
    "tp2, fp2, tn2, fn2 = calculate_confusion_matrix(gt_image2, diff_image2[0, :, :, 0])\n",
    "\n",
    "# 輸出混淆矩陣結果\n",
    "print(\"Image 1 Confusion Matrix:\")\n",
    "print(f\"TP: {tp1}, FP: {fp1}, TN: {tn1}, FN: {fn1}\")\n",
    "\n",
    "print(\"Image 2 Confusion Matrix:\")\n",
    "print(f\"TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d12a9ae-80e6-4319-85d9-11a2f1fc83ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth 1 shape: (800, 600)\n",
      "Groundtruth 2 shape: (800, 600)\n",
      "Prediction 1 shape: (800, 600)\n",
      "Prediction 2 shape: (800, 600)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Groundtruth 1 shape: {gt_image1.shape}\")\n",
    "print(f\"Groundtruth 2 shape: {gt_image2.shape}\")\n",
    "print(f\"Prediction 1 shape: {diff_image1[0, :, :, 0].shape}\")\n",
    "print(f\"Prediction 2 shape: {diff_image2[0, :, :, 0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ae14d-eeb4-4f88-8607-6a0280cb4c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
